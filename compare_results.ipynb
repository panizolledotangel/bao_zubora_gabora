{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zubora_gabora.experiment.experiment_loader import ExperimentLoader\n",
    "from scipy.stats import wilcoxon\n",
    "\n",
    "# Load the experiment\n",
    "ga_experiments = ExperimentLoader(\"experiments/ga/\").experiment_data\n",
    "aco_experiments = ExperimentLoader(\"experiments/aco/\").experiment_data\n",
    "num_experiments = len(ga_experiments[\"experiment_id\"].unique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show boxplots of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate boxplots of algorithms\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Concatenate the data\n",
    "ga_experiments[\"algorithm\"] = \"GA\"\n",
    "aco_experiments[\"algorithm\"] = \"ACO\"\n",
    "experiments = pd.concat([ga_experiments, aco_experiments], ignore_index=True)\n",
    "experiments[\"experiment_id\"] = experiments[\"experiment_id\"].astype(int)\n",
    "experiments[\"n_evaluations\"] = experiments[\"n_evaluations\"].astype(int)\n",
    "experiments[\"fitness\"] = experiments[\"fitness\"].astype(float)\n",
    "experiments[\"algorithm\"] = experiments[\"algorithm\"].astype(str)\n",
    "\n",
    "# Plot the fitness boxplot for each dataset\n",
    "for experiment_id in range(1,num_experiments+1):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.boxplot(x=\"algorithm\", y=\"fitness\", data=experiments[experiments[\"experiment_id\"] == experiment_id])\n",
    "    plt.title(f\"Fitness for dataset {experiment_id}\")\n",
    "    plt.show()\n",
    "    # Plot the number of evaluations boxplot for each dataset\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.boxplot(x=\"algorithm\", y=\"n_evaluations\", data=experiments[experiments[\"experiment_id\"] == experiment_id])\n",
    "    plt.title(f\"Number of evaluations for dataset {experiment_id}\")\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare each dataset using Wilcoxon signed-rank test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_better(a,b):\n",
    "  return a < b\n",
    "\n",
    "alpha = 0.05\n",
    "\n",
    "for i in range(1,num_experiments+1):\n",
    "  # Extract those where column experiment_id equals i\n",
    "  ga_experiment = ga_experiments[ga_experiments[\"experiment_id\"] == i]\n",
    "  aco_experiment = aco_experiments[aco_experiments[\"experiment_id\"] == i]\n",
    "\n",
    "  ga_fitness = ga_experiment[\"fitness\"]\n",
    "  aco_fitness = aco_experiment[\"fitness\"]\n",
    "\n",
    "  print(f\"Experiment {i}\")\n",
    "  print(f\"GA Fitness: {ga_fitness.mean()} +/- {ga_fitness.std()}\")\n",
    "  print(f\"ACO Fitness: {aco_fitness.mean()} +/- {aco_fitness.std()}\")\n",
    "\n",
    "  if is_better(ga_fitness.mean(), aco_fitness.mean()):\n",
    "    print(\"GA is better than ACO in Quality\")\n",
    "  else:\n",
    "    print(\"ACO is better than GA in Quality\")\n",
    "\n",
    "  res = wilcoxon(ga_fitness, aco_fitness)\n",
    "  if res.pvalue < alpha:\n",
    "    print(\"The difference is statistically significant\")\n",
    "  else:\n",
    "    print(\"The difference is not statistically significant\")\n",
    "\n",
    "  ga_evaluations = ga_experiment[\"n_evaluations\"]\n",
    "  aco_evaluations = aco_experiment[\"n_evaluations\"]\n",
    "\n",
    "  print(f\"GA Evaluations: {ga_evaluations.mean()} +/- {ga_evaluations.std()}\")\n",
    "  print(f\"ACO Evaluations: {aco_evaluations.mean()} +/- {aco_evaluations.std()}\")\n",
    "\n",
    "  if is_better(ga_evaluations.mean(), aco_evaluations.mean()):\n",
    "    print(\"GA is better than ACO in Speed\")\n",
    "  else:\n",
    "    print(\"ACO is better than GA in Speed\")\n",
    "  \n",
    "  res = wilcoxon(ga_evaluations, aco_evaluations)\n",
    "  if res.pvalue < alpha:\n",
    "    print(\"The difference is statistically significant\")\n",
    "  else:\n",
    "    print(\"The difference is NOT statistically significant\")\n",
    "  print(\"------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare all datasets at once using Friedman test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from stac.nonparametric_tests import friedman_aligned_ranks_test, shaffer_multitest\n",
    "\n",
    "def is_better(a,b):\n",
    "  return a < b\n",
    "\n",
    "alpha = 0.05\n",
    "\n",
    "names = [\"GA\", \"ACO\"]\n",
    "names_pos = dict(zip(names, range(len(names))))\n",
    "\n",
    "# Comparing quality\n",
    "ga_fitness = ga_experiment[\"fitness\"]\n",
    "aco_fitness = aco_experiment[\"fitness\"]\n",
    "\n",
    "_, p_value, rankings, pivots = friedman_aligned_ranks_test(ga_fitness, aco_fitness)\n",
    "\n",
    "if p_value < alpha:\n",
    "  d = dict(zip(names, pivots))\n",
    "  comp, _, _, adpval = shaffer_multitest(d)\n",
    "\n",
    "  for i, apv in enumerate(adpval):\n",
    "    if apv < alpha:\n",
    "      chunks = comp[i].split(\"vs\")\n",
    "\n",
    "      name_l = chunks[0].strip()\n",
    "      name_r = chunks[1].strip()\n",
    "\n",
    "      if is_better(rankings[names_pos[name_l]], rankings[names_pos[name_r]]):\n",
    "        print(f\"{name_l} is better than {name_r} in terms of Quality\")\n",
    "      else:\n",
    "        print(f\"{name_r} is better than {name_l} in terms of Quality\")\n",
    "    else:\n",
    "      print(f\"There is no difference in terms of Quality between {name_l} and {name_r}\")\n",
    "else:\n",
    "  print(f\"There is no difference in terms of Quality\")\n",
    "\n",
    "\n",
    "# Comparing speed\n",
    "ga_evaluations = ga_experiment[\"n_evaluations\"]\n",
    "aco_evaluations = aco_experiment[\"n_evaluations\"]\n",
    "\n",
    "_, p_value, rankings, pivots = friedman_aligned_ranks_test(ga_evaluations, aco_evaluations)\n",
    "\n",
    "if p_value < alpha:\n",
    "  d = dict(zip(names, pivots))\n",
    "  comp, _, _, adpval = shaffer_multitest(d)\n",
    "\n",
    "  for i, apv in enumerate(adpval):\n",
    "    if apv < alpha:\n",
    "      chunks = comp[i].split(\"vs\")\n",
    "\n",
    "      name_l = chunks[0].strip()\n",
    "      name_r = chunks[1].strip()\n",
    "\n",
    "      if is_better(rankings[names_pos[name_l]], rankings[names_pos[name_r]]):\n",
    "        print(f\"{name_l} is better than {name_r} in terms of Speed\")\n",
    "      else:\n",
    "        print(f\"{name_r} is better than {name_l} in terms of Speed\")\n",
    "    else:\n",
    "      print(f\"There is no difference in terms of Speed between {name_l} and {name_r}\")\n",
    "else:\n",
    "  print(f\"There is no difference in terms of Speed\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
